{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/c242587/Desktop/projects/git/ngboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGBoost: Natural Gradient Boosting for Probabilistic Prediction\n",
    "\n",
    "ngboost is a Python library that implements Natural Gradient Boosting, as described in [\"NGBoost: Natural Gradient Boosting for Probabilistic Prediction\"](https://stanfordmlgroup.github.io/projects/ngboost/). It is built on top of [Scikit-Learn](https://scikit-learn.org/stable/), and is designed to be scalable and modular with respect to choice of proper scoring rule, distribution, and base learners.\n",
    "\n",
    "Installation:\n",
    "\n",
    "```\n",
    "pip install --upgrade git+https://github.com/stanfordmlgroup/ngboost.git\n",
    "```\n",
    "\n",
    "## Basic Usage\n",
    "\n",
    "Probabilistic regression example on the Boston housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=3.6525 val_loss=0.0000 scale=0.5000 norm=3.4162\n",
      "[iter 100] loss=3.0940 val_loss=0.0000 scale=1.0000 norm=3.8410\n",
      "[iter 200] loss=2.4507 val_loss=0.0000 scale=2.0000 norm=4.0001\n",
      "[iter 300] loss=2.0341 val_loss=0.0000 scale=2.0000 norm=3.1840\n",
      "[iter 400] loss=1.8545 val_loss=0.0000 scale=1.0000 norm=1.4532\n",
      "Test MSE 11.307832273395793\n",
      "Test NLL 3.5564304554457395\n"
     ]
    }
   ],
   "source": [
    "from ngboost import NGBRegressor\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, Y = load_boston(True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "ngb = NGBRegressor().fit(X_train, Y_train)\n",
    "Y_preds = ngb.predict(X_test)\n",
    "Y_dists = ngb.pred_dist(X_test)\n",
    "\n",
    "# test Mean Squared Error\n",
    "test_MSE = mean_squared_error(Y_preds, Y_test)\n",
    "print('Test MSE', test_MSE)\n",
    "\n",
    "# test Negative Log Likelihood\n",
    "test_NLL = -Y_dists.logpdf(Y_test).mean()\n",
    "print('Test NLL', test_NLL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the estimated distributional parameters at a set of points is easy. This returns the predicted mean and standard deviation of the first five observations in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loc': array([15.64363402, 25.71892448, 24.55973364, 23.63940922, 26.42036304]),\n",
       " 'scale': array([1.44562276, 1.79561457, 1.51913637, 1.36714008, 1.18930348])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dists[0:5].params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions\n",
    "\n",
    "NGBoost can be used with a variety of distributions, broken down into those for regression (support on an infinite set) and those for classification (support on a finite set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Distribution | Parameters | Implemented Scores | Reference |\n",
    "| --- | --- | --- | --- |\n",
    "| `Normal` | `loc`, `scale` | `LogScore`, `CRPScore` | [`scipy.stats` normal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) |\n",
    "| `LogNormal` | `s`, `scale` | `LogScore`, `CRPScore` | [`scipy.stats` lognormal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.lognorm.html) |\n",
    "| `Exponential` | `scale` | `LogScore`, `CRPScore` | [`scipy.stats` exponential](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.expon.html) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression distributions can be used through the `NGBRegressor()` constructor by passing the appropriate class as the `Dist` argument. `Normal` is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngboost.distns import Exponential, Normal\n",
    "\n",
    "X, Y = load_boston(True)\n",
    "X_reg_train, X_reg_test, Y_reg_train, Y_reg_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "ngb_norm = NGBRegressor(Dist=Normal, verbose=False).fit(X_reg_train, Y_reg_train)\n",
    "ngb_exp = NGBRegressor(Dist=Exponential, verbose=False).fit(X_reg_train, Y_reg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two prediction methods for `NGBRegressor` objects: `predict()`, which returns point predictions as one would expect from a standard regressor, and `pred_dist()`, which returns a distribution object representing the conditional distribution of $Y|X=x_i$ at the points $x_i$ in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.40257675, 18.00645019, 18.22098446, 14.184202  , 22.77401087])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngb_norm.predict(X_reg_test)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.74254987, 18.02445107, 16.16793608, 13.90311783, 22.76919275])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngb_exp.predict(X_reg_test)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scale': array([12.74254987, 18.02445107, 16.16793608, 13.90311783, 22.76919275])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngb_exp.pred_dist(X_reg_test)[0:5].params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Survival Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGBoost supports analyses of right-censored data. Any distribution that can be used for regression in NGBoost can also be used for survival analysis in theory, but this requires the implementation of the right-censored version of the appropriate score. At the moment, `LogNormal` and `Exponential` have these scores implemented. To do survival analysis, use `NGBSurvival` and pass both the time-to-event (or censoring) and event indicator vectors to  `fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=1.2730 val_loss=0.0000 scale=8.0000 norm=4.7179\n",
      "[iter 100] loss=0.6011 val_loss=0.0000 scale=2.0000 norm=0.7369\n",
      "[iter 200] loss=0.3348 val_loss=0.0000 scale=4.0000 norm=0.8652\n",
      "[iter 300] loss=0.1987 val_loss=0.0000 scale=4.0000 norm=0.5071\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ngboost import NGBSurvival\n",
    "from ngboost.distns import LogNormal\n",
    "\n",
    "X, Y = load_boston(True)\n",
    "X_surv_train, X_surv_test, Y_surv_train, Y_surv_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# introduce administrative censoring to simulate survival data\n",
    "T_surv_train = np.minimum(Y_train, 30) # time of an event or censoring\n",
    "E_surv_train = Y_train > 30 # 1 if T[i] is the time of an event, 0 if it's a time of censoring\n",
    "\n",
    "ngb = NGBSurvival(Dist=LogNormal).fit(X_surv_train, T_surv_train, E_surv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Distribution | Parameters | Implemented Scores | Reference |\n",
    "| --- | --- | --- | --- |\n",
    "| `k_categorical(K)` | `p0`, `p1`... `p{K-1}` | `LogScore` | [Categorical distribution on Wikipedia](https://en.wikipedia.org/wiki/Categorical_distribution) |\n",
    "| `Bernoulli` | `p` | `LogScore` | [Bernoulli distribution on Wikipedia](https://en.wikipedia.org/wiki/Bernoulli_distribution) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification distributions can be used through the `NGBClassifier()` constructor by passing the appropriate class as the `Dist` argument. `Bernoulli` is the default and is equivalent to `k_categorical(2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns import k_categorical, Bernoulli\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(True)\n",
    "y[0:15] = 2 # artificially make this a 3-class problem instead of a 2-class problem\n",
    "X_cls_train, X_cls_test, Y_cls_train, Y_cls_test  = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "ngb_cat = NGBClassifier(Dist=k_categorical(3), verbose=False) # tell ngboost that there are 3 possible outcomes\n",
    "_ = ngb_cat.fit(X_cls_train, Y_cls_train) # Y should have only 3 values: {0,1,2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using NGBoost for classification, the outcome vector `Y` must consist only of integers from 0 to K-1, where K is the total number of classes. This is consistent with the classification standards in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NGBClassifier` objects have three prediction methods: `predict()` returns the most likely class, `predict_proba()` returns the class probabilities, and `pred_dist()` returns the distribution object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb_cat.predict(X_cls_test)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb_cat.predict_proba(X_cls_test)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb_cat.pred_dist(X_cls_test)[0:5].params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGBoost supports the log score (`LogScore`, also known as negative log-likelihood) and CRPS (`CRPScore`), although each score may not be implemented for each distribution. The score is specified by the `Score` argument in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngboost.scores import LogScore, CRPScore\n",
    "\n",
    "NGBRegressor(Dist=Exponential, Score=CRPScore, verbose=False).fit(X_reg_train, Y_reg_train)\n",
    "NGBClassifier(Dist=k_categorical(3), Score=LogScore, verbose=False).fit(X_cls_train, Y_cls_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGBoost can be used with any sklearn regressor as the base learner, specified with the `Base` argument. The default is a depth-3 regression tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "learner = DecisionTreeRegressor(criterion='friedman_mse', max_depth=5)\n",
    "\n",
    "NGBSurvival(Dist=Exponential, Score=CRPScore, Base=learner, verbose=False).fit(X_surv_train, T_surv_train, E_surv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate, number of estimators, and minibatch fraction are also easily adjusted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGBRegressor(n_estimators=100, learning_rate=0.01, minibatch_frac=0.5).fit(X_reg_train, Y_reg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staged Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fit NGBoost objects support staged prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb_cls = NGBClassifier(Dist=k_categorical(3), Score=LogScore, n_estimators=500, verbose=False).fit(X_cls_train, Y_cls_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, to get the predictions on the first 5 examples after fitting 415 base learners, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ngb_cls.staged_predict(X_cls_test)\n",
    "preds[415][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dists = ngb_cls.staged_pred_dist(X_cls_test)\n",
    "pred_dists[415][0:5].params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful in conjunction with tracking errors on a validation set, which you can do by passing the `X_val` and `Y_val` arguments and then inspecting the `.best_val_loss_itr` instance attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb = NGBRegressor()\n",
    "ngb.fit(X_reg_train, Y_reg_train, X_val=X_reg_test, Y_val=Y_reg_test) # use a validation set instead of test set here in your own work\n",
    "print(ngb.best_val_loss_itr)\n",
    "best_preds = ngb.predict(X_reg_test, max_iter=ngb.best_val_loss_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGBoost also has early stopping. If an integer `early_stopping_rounds` and a validation set (`X_val`,`Y_val`) are passed to `fit()`, the algorithm will stop running after the validation loss has increased for `early_stopping_rounds` of consecutive iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = NGBRegressor().fit(X_reg_train, Y_reg_train, X_val=X_reg_test, Y_val=Y_reg_test, early_stopping_rounds=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `sklearn` Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` methods are compatible with NGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "b1 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=2)\n",
    "b2 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=4)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 50],\n",
    "    'minibatch_frac': [1.0, 0.5],\n",
    "    'Base': [b1, b2]\n",
    "}\n",
    "\n",
    "ngb = NGBRegressor(Dist=Normal, verbose=False)\n",
    "\n",
    "grid_search = GridSearchCV(ngb, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_reg_train, Y_reg_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We caution users [against trying to interpret](https://www.google.com/search?q=mythos+of+interpretability&rlz=1C5CHFA_enUS831US831&oq=mythos+of+in&aqs=chrome.0.0j69i57j0l6.2088j0j7&sourceid=chrome&ie=UTF-8) too much from any machine learning model, but NGBoost does provide methods to interpret models fit with regression tree base learners. Since each parameter in the distribution is fit by a separate sequence of learners, there will be multiple model interpretation results, one for each parameter. The default distribution used is normal distribution so the following example shows results for the `loc` and `scale` parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb = NGBRegressor(verbose=False).fit(X_reg_train, Y_reg_train)\n",
    "\n",
    "## Feature importance for loc trees\n",
    "feature_importance_loc = ngb.feature_importances_[0]\n",
    "\n",
    "## Feature importance for scale trees\n",
    "feature_importance_scale = ngb.feature_importances_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_loc = pd.DataFrame({'feature':load_boston()['feature_names'], \n",
    "                       'importance':feature_importance_loc})\\\n",
    "    .sort_values('importance',ascending=False)\n",
    "df_scale = pd.DataFrame({'feature':load_boston()['feature_names'], \n",
    "                       'importance':feature_importance_scale})\\\n",
    "    .sort_values('importance',ascending=False)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,6))\n",
    "fig.suptitle(\"Feature importance plot for distribution parameters\", fontsize=17)\n",
    "sns.barplot(x='importance',y='feature',ax=ax1,data=df_loc, color=\"skyblue\").set_title('loc param')\n",
    "sns.barplot(x='importance',y='feature',ax=ax2,data=df_scale, color=\"skyblue\").set_title('scale param')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "## SHAP plot for loc trees\n",
    "explainer = shap.TreeExplainer(ngb, model_output=0) # use model_output = 1 for scale trees\n",
    "shap_values = explainer.shap_values(X_reg_train)\n",
    "shap.summary_plot(shap_values, X_reg_train, feature_names=load_boston()['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you work with NGBoost, you may want to experiment with distributions or scores that are not yet supported. Here we will walk through the process of implementing a new distribution or score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first order of business is to write the class for your new distribution. The distribution class must subclass the appropriate distribution type (either `RegressionDistn` or `ClassificationDistn`) and must implement methods for `fit()` and `sample()`. The scores compatible with the distribution should be stored in a class attribute called `score` and the number of parameters in an class attribute n_params. The class must also store the (internal) distributional parameters in a `_params` instance attribute. Additionally, regression distributions must implement a `mean()` method to support point prediction.\n",
    "\n",
    "We'll use the Laplace distribution as an example. The Laplace distribution has PDF $\\frac{1}{2b} e^{-\\frac{|x-\\mu|}{b}}$ with user-facing parameters $\\mu \\in \\mathbb{R}$ and $b > 0$, which we will call `loc` and `scale` to conform to the [`scipy.stats` implementation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.laplace.html). \n",
    "\n",
    "In NGBoost, all parameters must be represented internally in $\\mathbb R$, so we need to reparametrize $(\\mu, b)$ to, for instance, $(\\mu, \\log(b))$. The latter are the parameters we need to work with when we initialize a `Laplace` object and when implement the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import laplace as dist\n",
    "import numpy as np\n",
    "from ngboost.distns.distn import RegressionDistn\n",
    "from ngboost.scores import LogScore\n",
    "\n",
    "class LaplaceLogScore(LogScore): # will implement this later\n",
    "    pass\n",
    "\n",
    "class Laplace(RegressionDistn):\n",
    "\n",
    "    n_params = 2\n",
    "    scores = [LaplaceLogScore] # will implement this later\n",
    "\n",
    "    def __init__(self, params):\n",
    "        # save the parameters\n",
    "        self._params = params\n",
    "        \n",
    "        # create other objects that will be useful later\n",
    "        self.loc = params[0]\n",
    "        self.logscale = params[1]\n",
    "        self.scale = np.exp(params[1]) # since params[1] is log(scale)\n",
    "        self.dist = dist(loc=self.loc, scale=self.scale)\n",
    "\n",
    "    def fit(Y):\n",
    "        m, s = dist.fit(Y) # use scipy's implementation\n",
    "        return np.array([m, np.log(s)])\n",
    "\n",
    "    def sample(self, m):\n",
    "        return np.array([self.dist.rvs() for i in range(m)])\n",
    "    \n",
    "    def __getattr__(self, name): # gives us access to Laplace.mean() required for RegressionDist.predict()\n",
    "        if name in dir(self.dist):\n",
    "            return getattr(self.dist, name)\n",
    "        return None\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        return {'loc':self.loc, 'scale':self.scale}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit()` method is a class method that takes a vector of observations and fits a marginal distribution. Meanwhile, `sample()` should return a $m$ samples from $P(Y|X=x)$, each of which is a vector of `len(Y)`.\n",
    "\n",
    "Here we're taking advantage of the fact that `scipy.stats` already has the Laplace distribution implemented so we can steal its `fit()` method and put a thin wrapper around `rvs()` to get samples. We also use `__getattr__()` on the internal `scipy.stats` object to get access to its `mean()` method.\n",
    "\n",
    "Lastly, we write a convenience method `params()` that, when called, returns the distributional parameters as the user expects to see them, i.e. $(\\mu, b)$, not $(\\mu, \\log b)$.\n",
    "\n",
    "#### Implementing a Score for our Distribution\n",
    "\n",
    "Now we turn our attention to implementing a score that we can use with this distribution. We'll use the log score as an example. \n",
    "\n",
    "All implemented scores should subclass the appropriate score and implement three methods: \n",
    "\n",
    "* `score()` : the value of the score at the current parameters, given the data `Y`\n",
    "* `d_score()` : the derivative of the score at the current parameters, given the data `Y`\n",
    "* `metric()` : the value of the Riemannian metric at the current parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceLogScore(LogScore): \n",
    "    \n",
    "    def score(self, Y):\n",
    "        return -self.dist.logpdf(Y)\n",
    "\n",
    "    def d_score(self, Y):\n",
    "        D = np.zeros((len(Y), 2)) # first col is dS/d𝜇, second col is dS/d(log(b))\n",
    "        D[:, 0] = np.sign(self.logscale - Y)/self.scale\n",
    "        D[:, 1] = 1 - np.abs(self.logscale - Y)/self.scale\n",
    "        return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the attributes of an instance of `Laplace` are referenced using the `self.attr` notation even though we haven't said these will be attributes of the `LaplaceLogScore` class. When a user asks NGBoost to use the `Laplace` distribution with the `LogScore`, NGBoost will first find the implmentation of the log score that is compatible with `Laplace`, i.e. `LaplaceLogScore` and dynamically create a new class that has both the attributes of the distribution and the appropriate implementation of the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivatives with respect to [$\\log b$](https://www.wolframalpha.com/input/?i=d%2Fdb+-log%281%2F%282e%5Eb%29+e%5E%28-%7Cx-a%7C%2Fe%5Eb%29%29) and [$\\mu$](https://www.wolframalpha.com/input/?i=d%2Fda+-log%281%2F%282e%5Eb%29+e%5E%28-%7Cx-a%7C%2Fe%5Eb%29%29) are easily derived using, for instance, WolframAlpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we won't bother implementing `metric()`, which would return the current Fisher Information. The reason is that the NGBoost implmentation of `LogScore` has a default `metric()` method that uses a Monte Carlo method to approximate the Fisher Information using the `gradient()` method and the distribution's `sample()` method (that's why we needed to implement `sample()`). By inhereting from `LogScore()`, not only can NGBoost find our implementation for the Laplace distribution, it can also fall back on the defualt `metric()` method. \n",
    "\n",
    "CRPScore does not have a default metric method, so implementations of that score should have an explicit `metric()` method. The math is sometimes tricky to derive the Riemannian metrics. Also note that NGBoost will run faster if you provide an explicit implementation of `metric()` for implementations of `LogScore`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceLogScore(LogScore): \n",
    "    \n",
    "    def score(self, Y):\n",
    "        return -self.dist.logpdf(Y)\n",
    "\n",
    "    def d_score(self, Y):\n",
    "        D = np.zeros((len(Y), 2)) # first col is dS/d𝜇, second col is dS/d(log(b))\n",
    "        D[:, 0] = -np.sign(self.logscale - Y)/self.scale\n",
    "        D[:, 1] = 1 - np.abs(self.logscale - Y)/self.scale\n",
    "        return D\n",
    "\n",
    "class Laplace(RegressionDistn):\n",
    "\n",
    "    n_params = 2\n",
    "    scores = [LaplaceLogScore] \n",
    "\n",
    "    def __init__(self, params):\n",
    "        # save the parameters\n",
    "        self._params = params\n",
    "        \n",
    "        # create other objects that will be useful later\n",
    "        self.loc = params[0]\n",
    "        self.logscale = params[1]\n",
    "        self.scale = np.exp(params[1]) # since params[1] is log(scale)\n",
    "        self.dist = dist(loc=self.loc, scale=self.scale)\n",
    "\n",
    "    def fit(Y):\n",
    "        m, s = dist.fit(Y) # use scipy's implementation\n",
    "        return np.array([m, np.log(s)])\n",
    "\n",
    "    def sample(self, m):\n",
    "        return np.array([self.dist.rvs() for i in range(m)])\n",
    "    \n",
    "    def __getattr__(self, name): # gives us access to Laplace.mean() required for RegressionDist.predict()\n",
    "        if name in dir(self.dist):\n",
    "            return getattr(self.dist, name)\n",
    "        return None\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        return {'loc':self.loc, 'scale':self.scale}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can test our method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb = NGBRegressor(Dist=Laplace, Score=LogScore).fit(X_reg_train, Y_reg_train)\n",
    "Y_preds = ngb.predict(X_test)\n",
    "Y_dists = ngb.pred_dist(X_test)\n",
    "\n",
    "# test Mean Squared Error\n",
    "test_MSE = mean_squared_error(Y_preds, Y_test)\n",
    "print('Test MSE', test_MSE)\n",
    "\n",
    "# test Negative Log Likelihood\n",
    "test_NLL = -Y_dists.logpdf(Y_test).mean()\n",
    "print('Test NLL', test_NLL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dig into the source of `ngboost.distns` to find more examples. If you write and test your own distribution, please contribute it to NGBoost by making a pull request!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Censored Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make your distribution suitable for use in surival analysis if you implement a censored version of the score. The signature for the `score()`, `d_score()` and `metric()` methods should be the same, but they should expect `Y` to be indexable into two arrays like `E, T = Y[\"Event\"], Y[\"Time\"]`. Furthermore, any censored scores should be linked to the distribution class definition via a class attribute called `censored_scores` instead of `scores`. \n",
    "\n",
    "Since censored scores are more general than their standard counterparts (fully observed data is a specific case of censored data), if you implement a censored score in NGBoost, it will automatically become available as a useable score for standard regression analysis. No need to implement the regression score seperately or register it in the `scores` class attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen how to *implement* an existing score for a new distribution, but making a new score altogether in NGBoost is also easy: just make a new class that subclasses `Score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngboost.scores import Score\n",
    "\n",
    "class SphericalScore(Score):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Distribution-specific implemenations of this score (e.g. `LaplaceSphericalScore`) should subclass `SphericalScore`. The implementations of `LogScore` and `CRPScore` are in `ngboost.scores` for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
